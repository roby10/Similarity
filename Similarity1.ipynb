{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7G6RzkXxbs8"
      },
      "source": [
        "Name: Robert S.\r\n",
        "\r\n",
        "Algorithm idea:\r\n",
        "\r\n",
        "1.Restrict the data we compute similarity on, i chose to create a list of potential candidates by their number of words matching with the input.\r\n",
        "We keep just the candidates that have the maximum numbers of words matching.\r\n",
        "\r\n",
        "Example: \r\n",
        "Candidates [7734: 2, 83992: 2, 902:1 ... ]\r\n",
        "We only keep lines with the ids 7734 and 83992 that match exactly 2 words.\r\n",
        "Using threshold=5 means adding 902 and 4 others that match less words.\r\n",
        "\r\n",
        "Example: \r\n",
        "\r\n",
        "query: \"baring eastern trust\"\r\n",
        "\r\n",
        "potential candidates : [\"baring uk growth trust\", \"the r baring childrens trust\", \"the j baring childrens trust\"]\r\n",
        "\r\n",
        "In all 3 candidates there are 2 words matching (\"baring\" and \"trust\")\r\n",
        "\r\n",
        "2.Use a pretrained embedding (sentence embedding or word embedding with sum/avg over words) to compute cosine similarity between the candidates and the query.\r\n",
        "\r\n",
        "3.Sort by similarity and return the most similar one.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsKsIhDr0PEH"
      },
      "source": [
        "Additional findings and results:\r\n",
        "\r\n",
        "1.Using various embeddings:\r\n",
        "\r\n",
        "-Universal Sentence Encoder Multilingual (512 dim vector as output) -> ~82% on train data\r\n",
        "\r\n",
        "-Universal Sentence Encoder Multilingual Large model (512 dim vector as output) -> 81.8% on train data\r\n",
        "\r\n",
        "-Universal Sentence Encoder (512 dim vector as output) -> ~80% on train data\r\n",
        "\r\n",
        "-Universal Sentence Encoder Large model (512 dim vector as output) -> ~78% on train data\r\n",
        "\r\n",
        "-Wiki words (250 dim vector as output) -> 75.6% on train data\r\n",
        "\r\n",
        "-NNLM-en-news (128 dim vector as output) -> 73% on train data\r\n",
        "\r\n",
        "-Gnews-Swivel (20 dim vector as output) -> 73% on train data\r\n",
        "\r\n",
        "\r\n",
        "2.Candidates distribution over train dataset:\r\n",
        "\r\n",
        "(chance that the result is in the candidates , threshold = add 0/5/10/25 more candidates from the sorted list based on the number of words matchin)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "-len + 0 threshold = 0.899 \r\n",
        "\r\n",
        "![](https://drive.google.com/uc?export=view&id=1j99GUy2wgHDi1YiVSdVCHXlVYNPvtkBN)\r\n",
        "\r\n",
        "-len + 5 threshold = 0.943\r\n",
        "\r\n",
        "![](https://drive.google.com/uc?export=view&id=1exy7jM6eclUO4jvOT-w0wHSeRfHRF29X)\r\n",
        "\r\n",
        "-len + 10 threshold = 0.952\r\n",
        "\r\n",
        "![](https://drive.google.com/uc?export=view&id=1hTbxH72Mn0PKsw6QpYkcS3FlR_2LPPyy)\r\n",
        "\r\n",
        "-len + 25 threshold = 0.967\r\n",
        "\r\n",
        "![](https://drive.google.com/uc?export=view&id=1NxErIynVYUTu4bo7ubOq_QeEPZhi6fQK)\r\n",
        "\r\n",
        "\r\n",
        "-Having higher threshold doesn't work that well with the similarity metric , we obtain worse results than 0 threshold.\r\n",
        "\r\n",
        "-It would be useful to increase the threshold if the embedding is better.\r\n",
        "\r\n",
        "-Bert/Albert/ and other language models don't work that well, the hypothesis is that the larger the embedding size the worse the results for cosine similarity (for example with bert lot of candidates have undistinguishable probabilities 0.9998, 0.9997 .. etc)\r\n",
        "\r\n",
        "-Speed:~10-12 queries/sec on cpu\r\n",
        "\r\n",
        "-Computing embeddings on batches from base and keeping the text with highest similarity metric takes too much time (batch=10k with Universal sentence encoder takes ~20 secs on cpu -> 20sec * 1.7mil/10k = 20*170 ~= 1h per query\r\n",
        "\r\n",
        "\r\n",
        "-Further improvements: \r\n",
        "-simhash for misspelling : \"bearing\" and \"bearings\" should be bucketed in the same hash bucket with simhash or other types of hasing functions that generate same hasing bucket for small changes.\r\n",
        "-autoencoder or VAE applied on the base and use the latent layer as embedding to compute cosine similarity\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woIMrORYS9Bl"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import io\r\n",
        "import time\r\n",
        "import re\r\n",
        "import sys\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_text as text\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import torch\r\n",
        "\r\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXQd92hvTBzc"
      },
      "source": [
        "###define your data paths here\r\n",
        "base = 'C:\\\\Users\\\\roby10\\\\Desktop\\\\French\\\\public_dat\\\\base.csv'\r\n",
        "train  = 'C:\\\\Users\\\\roby10\\\\Desktop\\\\French\\\\public_dat\\\\train.csv'\r\n",
        "\r\n",
        "val =   'C:\\\\Users\\\\roby10\\\\Desktop\\\\French\\\\public_dat\\\\val.csv'\r\n",
        "valOutput = 'C:\\\\Users\\\\roby10\\\\Desktop\\\\French\\\\public_dat\\\\submission_val.csv'\r\n",
        "\r\n",
        "test =  'C:\\\\Users\\\\roby10\\\\Desktop\\\\French\\\\public_dat\\\\test.csv'\r\n",
        "testOutput =  'C:\\\\Users\\\\roby10\\\\Desktop\\\\French\\\\public_dat\\\\submission_test.csv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwGyhBf5TaVc",
        "outputId": "a4f2e917-8d5a-4fee-dad3-18c514ffc349"
      },
      "source": [
        "def clean(text):\r\n",
        "    text = text.lower()\r\n",
        "    text = re.sub(r'\"', '', text)\r\n",
        "    text = re.sub(r'[-|,+éèêëàâäôöîï]', '', text)\r\n",
        "    return text\r\n",
        "\r\n",
        "idx = 0 \r\n",
        "wordDict = {}\r\n",
        "lines = []\r\n",
        "\r\n",
        "with io.open(base, mode=\"r\", encoding=\"utf-8\") as f:\r\n",
        "  header = f.readline()\r\n",
        "\r\n",
        "  while True:\r\n",
        "    line = f.readline().rstrip()\r\n",
        "\r\n",
        "    if not line:\r\n",
        "      break\r\n",
        "    \r\n",
        "    name, lei = line.rsplit(',', 1)\r\n",
        "    name = clean(name)\r\n",
        "    lines.append((name,lei))\r\n",
        "\r\n",
        "    ###contruct a dict where words are the keys and id of the lines are the values\r\n",
        "    ###example : \r\n",
        "    ###  id 1 : \"barings trust\"\r\n",
        "    ###  id 2 : \"barings fund\"\r\n",
        "    ###  dict {\"barings\": [1,2], \"trust\": [1], \"fund\":[2]}\r\n",
        "    ###this is used for calculating the numbers of words matching\r\n",
        "    for word in set(name.split(' ')):\r\n",
        "      if word not in wordDict.keys():\r\n",
        "        wordDict[word] = [idx]\r\n",
        "      else:\r\n",
        "        wordDict[word].append(idx)\r\n",
        "    \r\n",
        "    idx += 1\r\n",
        "  \r\n",
        "    if idx % 100000 == 0:\r\n",
        "      print(idx)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "200000\n",
            "300000\n",
            "400000\n",
            "500000\n",
            "600000\n",
            "700000\n",
            "800000\n",
            "900000\n",
            "1000000\n",
            "1100000\n",
            "1200000\n",
            "1300000\n",
            "1400000\n",
            "1500000\n",
            "1600000\n",
            "1700000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKhDYLSdD90s"
      },
      "source": [
        "####Maxw = 54 words\r\n",
        "####Maxc = 388 chacaters\r\n",
        "#print(np.max([len(x) for (x,y) in lines]))\r\n",
        "#print(np.max([len(x.split(' ')) for (x,y) in lines]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6faGUg-1Dgob",
        "outputId": "5871be10-2e9b-4dbb-e99f-cd21e592a265"
      },
      "source": [
        "print(str(sys.getsizeof(wordDict)/1000000) + \" MB\")\r\n",
        "print(str(sys.getsizeof(lines)/1000000) + \" MB\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41.943136 MB\n",
            "15.6734 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1vUxje9Dw8p"
      },
      "source": [
        "def findMatch(text, threshold=0):\r\n",
        "  text = text.lower()\r\n",
        "  words = text.split(' ')\r\n",
        "  candidateWords = []\r\n",
        "  k = wordDict.keys()\r\n",
        "\r\n",
        "\r\n",
        "  for word in words:    \r\n",
        "    tmpCand = []\r\n",
        "\r\n",
        "    if word in k:\r\n",
        "      for lineID in wordDict[word]:\r\n",
        "        tmpCand.append(lineID)\r\n",
        "      candidateWords.append(tmpCand)\r\n",
        "    else:\r\n",
        "      continue\r\n",
        "  \r\n",
        "  ###compute occurrence probability\r\n",
        "\r\n",
        "  allPossible = []\r\n",
        "  for lst in candidateWords:\r\n",
        "    allPossible.extend(lst)\r\n",
        "\r\n",
        "  candidateProb = {}\r\n",
        "  for candidate in allPossible:\r\n",
        "    if candidate not in candidateProb.keys():\r\n",
        "      candidateProb[candidate] = 1\r\n",
        "    else:\r\n",
        "      candidateProb[candidate] += 1\r\n",
        "\r\n",
        "  ###TODO: double pass: get max , filter == max in case we don't use threshold\r\n",
        "  candidateProb = dict(sorted(candidateProb.items(), key=lambda item: item[1], reverse=True))\r\n",
        "  \r\n",
        "  maxV = list(candidateProb.values())[0]\r\n",
        "  filteredCand = [x for (x,y) in candidateProb.items() if y == maxV]\r\n",
        "\r\n",
        "  ### add additional items based on threshold value\r\n",
        "  idx = 0\r\n",
        "  for (x,y) in candidateProb.items():\r\n",
        "    if idx >= threshold:\r\n",
        "      break\r\n",
        "    if y != maxV:\r\n",
        "      filteredCand.append(x)\r\n",
        "      idx += 1\r\n",
        "\r\n",
        "  candTexts = [lines[x][0] for x in filteredCand]\r\n",
        "  filteredLeis = [lines[x][1] for x in filteredCand]\r\n",
        "\r\n",
        "  inputEmb = embed([text])\r\n",
        "  embeddings = embed(candTexts)\r\n",
        "\r\n",
        "  res = cosine_similarity(inputEmb, embeddings)\r\n",
        "\r\n",
        "  ###display the probabilities and texts based on similarity\r\n",
        "  '''\r\n",
        "  tmpData = pd.DataFrame()\r\n",
        "  tmpData['name'] = candTexts\r\n",
        "  tmpData['prob'] = res[0]\r\n",
        "  tmpData['leis'] = filteredLeis\r\n",
        "  tmpData.sort_values(by=['prob'], inplace=True, ascending=False)\r\n",
        "\r\n",
        "  \r\n",
        "  print(text)\r\n",
        "  print(tmpData)\r\n",
        "  '''\r\n",
        "  \r\n",
        "  return filteredLeis[np.argmax(res)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYwBU9HihUnx",
        "outputId": "445461a0-a79b-4924-faca-fc97fe8f1c3c"
      },
      "source": [
        "allSamples = 0\r\n",
        "correctSamples = 0\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "with io.open(train, mode=\"r\", encoding=\"utf-8\") as f:\r\n",
        "  header = f.readline()\r\n",
        "\r\n",
        "  while True:\r\n",
        "    line = f.readline().rstrip()\r\n",
        "\r\n",
        "    if not line:\r\n",
        "      break\r\n",
        "   \r\n",
        "    name, lei = line.rsplit(',', 1)\r\n",
        "    predLei = findMatch(name)\r\n",
        "\r\n",
        "    if predLei == lei:\r\n",
        "      correctSamples += 1\r\n",
        "\r\n",
        "    allSamples += 1\r\n",
        "\r\n",
        "    ####debug purpose\r\n",
        "    #if allSamples > 2:\r\n",
        "    #  assert 0\r\n",
        "\r\n",
        "duration = time.time() - start\r\n",
        "print(\"Avg results: \" + str(float(correctSamples/allSamples)) + \" \" + str(allSamples) + \" samples took \" + str(duration) + \"secs     speed: \" + str(allSamples/duration) + \" samples/sec\" )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg results: 0.8015122873345936 529 samples took 43.45313549041748secs     speed: 12.17403517674019 samples/sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5DLictD6v-E"
      },
      "source": [
        "#####predict on validation data\r\n",
        "allSamples = 0\r\n",
        "\r\n",
        "fout = io.open(valOutput, mode=\"w\", encoding=\"utf-8\")\r\n",
        "fout.write(\"lei\\n\")\r\n",
        "with io.open(val, mode=\"r\", encoding=\"utf-8\") as f:\r\n",
        "  header = f.readline()\r\n",
        "\r\n",
        "  while True:\r\n",
        "    line = f.readline().rstrip()\r\n",
        "\r\n",
        "    if not line:\r\n",
        "      break\r\n",
        "\r\n",
        "    predLei = findMatch(line)\r\n",
        "\r\n",
        "    fout.write(predLei + \"\\n\")\r\n",
        "    allSamples += 1\r\n",
        "fout.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyz-FqYZC7TH"
      },
      "source": [
        "#####predict on test data\r\n",
        "allSamples = 0\r\n",
        "\r\n",
        "fout = io.open(testOutput, mode=\"w\", encoding=\"utf-8\")\r\n",
        "fout.write(\"lei\\n\")\r\n",
        "with io.open(test, mode=\"r\", encoding=\"utf-8\") as f:\r\n",
        "  header = f.readline()\r\n",
        "\r\n",
        "  while True:\r\n",
        "    line = f.readline().rstrip()\r\n",
        "\r\n",
        "    if not line:\r\n",
        "      break\r\n",
        "\r\n",
        "    predLei = findMatch(line)\r\n",
        "    fout.write(predLei + \"\\n\")\r\n",
        "    allSamples += 1\r\n",
        "fout.close()"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}